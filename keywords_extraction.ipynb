{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ALTEGRAD.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jl6yyKL1sxBC",
        "colab_type": "code",
        "outputId": "fb2fd6e0-69f6-444f-916a-005ebea62a22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        " !pip3 install 'python-igraph'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-igraph in /usr/local/lib/python3.6/dist-packages (0.7.1.post6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOXzjoSWJf_b",
        "colab_type": "code",
        "outputId": "b2338142-3657-4408-9c7f-1b1a5d650d6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('maxent_treebank_pos_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "#regarder ligne par ligne ordre de grander de cb d'operation on doit faire et on garde le pire (par ligne)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwU0phHoZTS4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re \n",
        "import itertools\n",
        "import operator\n",
        "import copy\n",
        "import igraph\n",
        "import heapq\n",
        "import nltk\n",
        "# requires nltk 3.2.1\n",
        "from nltk import pos_tag # nltk.download('maxent_treebank_pos_tagger')\n",
        "\n",
        "def clean_text_simple(text, my_stopwords, punct, remove_stopwords=True, pos_filtering=True, stemming=True):\n",
        "    text = text.lower()\n",
        "    text = ''.join(l for l in text if l not in punct) # remove punctuation (preserving intra-word dashes)\n",
        "    text = re.sub(' +',' ',text) # strip extra white space\n",
        "    text = text.strip() # strip leading and trailing white space\n",
        "    tokens = text.split(' ') # tokenize (split based on whitespace)\n",
        "    if pos_filtering == True:\n",
        "        # POS-tag and retain only nouns and adjectives\n",
        "        tagged_tokens = pos_tag(tokens)\n",
        "        tokens_keep = []\n",
        "        for item in tagged_tokens:\n",
        "            if (\n",
        "            item[1] == 'NN' or\n",
        "            item[1] == 'NNS' or\n",
        "            item[1] == 'NNP' or\n",
        "            item[1] == 'NNPS' or\n",
        "            item[1] == 'JJ' or\n",
        "            item[1] == 'JJS' or\n",
        "            item[1] == 'JJR'\n",
        "            ):\n",
        "                tokens_keep.append(item[0])\n",
        "        tokens = tokens_keep\n",
        "    if remove_stopwords:\n",
        "        tokens = [token for token in tokens if token not in my_stopwords]\n",
        "    if stemming:\n",
        "        stemmer = nltk.stem.PorterStemmer()\n",
        "        tokens_stemmed = list()\n",
        "        for token in tokens:\n",
        "            tokens_stemmed.append(stemmer.stem(token))\n",
        "        tokens = tokens_stemmed\n",
        "    \n",
        "    return(tokens)\n",
        "\n",
        "\n",
        "def terms_to_graph(terms, window_size):\n",
        "    '''This function returns a directed, weighted igraph from lists of list of terms (the tokens from the pre-processed text)\n",
        "    e.g., ['quick','brown','fox']\n",
        "    Edges are weighted based on term co-occurence within a sliding window of fixed size 'w'\n",
        "    fenetre glissante met à jour liste de voisins, si deux mots apparaissent dans la meme fenetre, occurence +=1\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    from_to = {}\n",
        "\n",
        "    w = min(window_size, len(terms))\n",
        "    # create initial complete graph (first w terms)\n",
        "    terms_temp = terms[0:w]\n",
        "    indexes = list(itertools.combinations(range(w), r=2))\n",
        "\n",
        "    new_edges = []\n",
        "\n",
        "    for my_tuple in indexes:\n",
        "        new_edges.append(tuple([terms_temp[i] for i in my_tuple]))\n",
        "    for new_edge in new_edges:\n",
        "        if new_edge in from_to:\n",
        "            from_to[new_edge] += 1\n",
        "        else:\n",
        "            from_to[new_edge] = 1\n",
        "\n",
        "    # then iterate over the remaining terms\n",
        "    for i in range(w, len(terms)):\n",
        "        # term to consider\n",
        "        considered_term = terms[i]\n",
        "        # all terms within sliding window\n",
        "        terms_temp = terms[(i - w + 1):(i + 1)]\n",
        "\n",
        "        # edges to try\n",
        "        candidate_edges = [] #possible nouvelles edges \n",
        "        for p in range(w - 1):\n",
        "            candidate_edges.append((terms_temp[p], considered_term)) #terms temporaires = fenetre précédente\n",
        "\n",
        "        for try_edge in candidate_edges:\n",
        "\n",
        "            # if not self-edge\n",
        "            if try_edge[1] != try_edge[0]:\n",
        "\n",
        "                # if edge has already been seen, update its weight\n",
        "                if try_edge in from_to:\n",
        "                    from_to[try_edge] += 1\n",
        "\n",
        "                # if edge has never been seen, create it and assign it a unit weight\n",
        "                else:\n",
        "                    from_to[try_edge] = 1\n",
        "\n",
        "    # create empty graph\n",
        "    g = igraph.Graph(directed=True)\n",
        "\n",
        "    # add vertices\n",
        "    g.add_vertices(sorted(set(terms)))\n",
        "\n",
        "    # add edges, direction is preserved since the graph is directed\n",
        "    g.add_edges(list(from_to.keys()))\n",
        "    \n",
        "    # set edge and vertice weights\n",
        "    g.es['weight'] = list(from_to.values()) # based on co-occurence within sliding window\n",
        "    g.vs['weight'] = g.strength(weights=list(from_to.values())) # weighted degree\n",
        "\n",
        "    return (g)\n",
        "\n",
        "''' As long as we have core left, we remove the element with the lowest value  '''\n",
        "def core_dec(g,weighted):\n",
        "    '''(un)weighted k-core decomposition'''\n",
        "    # work on clone of g to preserve g \n",
        "    gg = copy.deepcopy(g)\n",
        "    if not weighted:\n",
        "        gg.vs['weight'] = gg.strength() # overwrite the 'weight' vertex attribute with the unweighted degrees\n",
        "    # initialize dictionary that will contain the core numbers\n",
        "    cores_g = dict(zip(gg.vs['name'],[0]*len(gg.vs)))\n",
        "   # print(gg.vs['weight'])\n",
        "    while len(gg.vs) > 0:\n",
        "        # find index of lowest degree vertex\n",
        "        min_degree = min(gg.vs['weight']) #minimum degre\n",
        "        #print('MIN ', min_degree)\n",
        "        index_top = gg.vs['weight'].index(min_degree) #index correspondant au poids min\n",
        "        name_top = gg.vs[index_top]['name'] #mot correspondant au poids min\n",
        "        # get names of its neighbors\n",
        "        neighbors = gg.vs[gg.neighbors(index_top)]['name']\n",
        "        # exclude self-edges\n",
        "        neighbors = [elt for elt in neighbors if elt!=name_top]\n",
        "        # set core number of lowest degree vertex as its degree\n",
        "        cores_g[name_top] = min_degree\n",
        "        ### delete top vertex and its incident edges ###\n",
        "        gg.delete_vertices(gg.vs.select(name = name_top))\n",
        "        if neighbors:\n",
        "            if weighted: #On va parcourir les voisins du sommet supprimé et mettre à jour leur poids avec strenght()\n",
        "                new_degrees = gg.strength(weights = gg.es['weight'])           \n",
        "             \n",
        "            else:\n",
        "                #Parcourir les sommets voisins et compter leurs voisins respectifs\n",
        "                new_degrees = gg.strength()\n",
        "             # iterate over neighbors of top element\n",
        "            for neigh in neighbors:\n",
        "                index_n = gg.vs['name'].index(neigh)\n",
        "                gg.vs[index_n]['weight'] = max(min_degree,new_degrees[index_n])  \n",
        "                \n",
        "    return(cores_g)\n",
        "\n",
        "\n",
        "def accuracy_metrics(candidate, truth):\n",
        "    # true positives ('hits') are both in candidate and in truth\n",
        "    tp = len(set(candidate).intersection(truth))\n",
        "    # false positives a.k.a. false alarms are in candidate but not in truth\n",
        "    fp = len([element for element in candidate if element not in truth])\n",
        "    ### compute false negatives a.k.a. misses, save results as 'fn'\n",
        "    fn = len([element for element in truth if element not in candidate])\n",
        "    ### compute precision and recall as a function of 'tp', 'fp' and 'fn', save results as 'prec' and 'rec' ###\n",
        "    prec = tp/(tp+fp)\n",
        "    rec = tp/(tp+fn)\n",
        "    if prec+rec != 0:\n",
        "        f1 = 2*prec*rec/(prec+rec)\n",
        "    else:\n",
        "        f1 = 0\n",
        "    \n",
        "    return (prec, rec, f1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzL2lW2DGAJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--eNeOUSEQ5C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ouc6xSLZTIS",
        "colab_type": "code",
        "outputId": "7dc8c04c-2a6d-430f-f3c6-4262177de734",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "\n",
        "## GOW TOY ##\n",
        "import string\n",
        "!nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "#import os\n",
        "#os.chdir() # to change working directory to where functions live\n",
        "# import custom functions\n",
        "#from library import clean_text_simple, terms_to_graph, core_dec\n",
        "\n",
        "stpwds = stopwords.words('english')\n",
        "punct = string.punctuation.replace('-', '')\n",
        "\n",
        "my_doc = 'A method for solution of systems of linear algebraic equations \\\n",
        "with m-dimensional lambda matrices. A system of linear algebraic \\\n",
        "equations with m-dimensional lambda matrices is considered. \\\n",
        "The proposed method of searching for the solution of this system \\\n",
        "lies in reducing it to a numerical system of a special kind.'\n",
        "\n",
        "my_doc = my_doc.replace('\\n', '')\n",
        "\n",
        "# pre-process document\n",
        "my_tokens = clean_text_simple(my_doc,my_stopwords=stpwds,punct=punct)\n",
        "\n",
        "g = terms_to_graph(my_tokens, 4) #window of size 4\n",
        "\n",
        "# number of edges\n",
        "print(len(g.es))\n",
        "\n",
        "# the number of nodes should be equal to the number of unique terms\n",
        "len(g.vs) == len(set(my_tokens))\n",
        "\n",
        "edge_weights = []\n",
        "for edge in g.es:\n",
        "    source = g.vs[edge.source]['name']\n",
        "    target = g.vs[edge.target]['name']\n",
        "    weight = edge['weight']\n",
        "    edge_weights.append([source, target, weight])\n",
        "\n",
        "print(edge_weights)\n",
        "\n",
        "for w in range(2,10):\n",
        "    g = terms_to_graph(my_tokens, w)\n",
        "    ### print density of g ###\n",
        "    print('density ',g.density(loops=False))\n",
        "\n",
        "# decompose g\n",
        "core_numbers = core_dec(g,True)\n",
        "print('core numbers', core_numbers)\n",
        "print('coreness',g.coreness(mode=3))\n",
        "### compare 'core_numbers' with the output of the .coreness() igraph method ###\n",
        "\n",
        "# retain main core as keywords\n",
        "max_c_n = max(core_numbers.values())\n",
        "keywords = [kwd for kwd, c_n in core_numbers.items() if c_n == max_c_n]\n",
        "print(\"keys\",keywords)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: -c: line 0: syntax error near unexpected token `'stopwords''\n",
            "/bin/bash: -c: line 0: `nltk.download('stopwords')'\n",
            "42\n",
            "[['method', 'solut', 2], ['method', 'system', 2], ['method', 'linear', 1], ['solut', 'system', 3], ['solut', 'linear', 1], ['system', 'linear', 2], ['solut', 'algebra', 1], ['system', 'algebra', 2], ['linear', 'algebra', 2], ['system', 'equat', 2], ['linear', 'equat', 2], ['algebra', 'equat', 2], ['linear', 'm-dimension', 2], ['algebra', 'm-dimension', 2], ['equat', 'm-dimension', 2], ['algebra', 'lambda', 2], ['equat', 'lambda', 2], ['m-dimension', 'lambda', 2], ['equat', 'system', 1], ['m-dimension', 'system', 1], ['lambda', 'system', 1], ['m-dimension', 'linear', 1], ['lambda', 'linear', 1], ['lambda', 'algebra', 1], ['equat', 'matric', 1], ['m-dimension', 'matric', 1], ['lambda', 'matric', 1], ['m-dimension', 'method', 1], ['lambda', 'method', 1], ['matric', 'method', 1], ['lambda', 'solut', 1], ['matric', 'solut', 1], ['matric', 'system', 1], ['method', 'numer', 1], ['solut', 'numer', 1], ['system', 'numer', 1], ['numer', 'system', 1], ['system', 'special', 2], ['numer', 'special', 1], ['numer', 'kind', 1], ['system', 'kind', 1], ['special', 'kind', 1]]\n",
            "density  0.10606060606060606\n",
            "density  0.21212121212121213\n",
            "density  0.3181818181818182\n",
            "density  0.41666666666666663\n",
            "density  0.5227272727272727\n",
            "density  0.5833333333333334\n",
            "density  0.6287878787878788\n",
            "density  0.6666666666666666\n",
            "core numbers {'algebra': 19.0, 'equat': 19.0, 'kind': 8.0, 'lambda': 19.0, 'linear': 19.0, 'm-dimension': 19.0, 'matric': 12.0, 'method': 18.0, 'numer': 9.0, 'solut': 18.0, 'special': 8.0, 'system': 19.0}\n",
            "coreness [13, 13, 7, 13, 13, 13, 9, 13, 9, 13, 7, 13]\n",
            "keys ['algebra', 'equat', 'lambda', 'linear', 'm-dimension', 'system']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juqLD2DoICL_",
        "colab_type": "code",
        "outputId": "506174f5-4763-4806-a4a3-261d9b0c6d60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 936
        }
      },
      "source": [
        "###KEY EXTRACTION###\n",
        "\n",
        "import os\n",
        "import string\n",
        "import re \n",
        "import operator\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#from library import clean_text_simple,terms_to_graph,core_dec,accuracy_metrics\n",
        "\n",
        "stemmer = nltk.stem.PorterStemmer()\n",
        "stpwds = stopwords.words('english')\n",
        "punct = string.punctuation.replace('-', '')\n",
        "\n",
        "##################################\n",
        "# read and pre-process abstracts #\n",
        "##################################\n",
        "\n",
        "path_to_abstracts =r\"for_moodle/data/Hulth2003testing/abstracts\"\n",
        "abstract_names = sorted(os.listdir(path_to_abstracts))\n",
        "\n",
        "abstracts = []\n",
        "for counter,filename in enumerate(abstract_names):\n",
        "    # read file\n",
        "    with open(path_to_abstracts + '/' + filename, 'r') as my_file: \n",
        "        text = my_file.read().splitlines()\n",
        "    text = ' '.join(text)\n",
        "    # remove formatting\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    abstracts.append(text)\n",
        "    \n",
        "    if counter % round(len(abstract_names)/5) == 0:\n",
        "        print(counter, 'files processed')\n",
        "\n",
        "abstracts_cleaned = []\n",
        "for counter,abstract in enumerate(abstracts):\n",
        "    my_tokens = clean_text_simple(abstract,my_stopwords=stpwds,punct=punct)\n",
        "    abstracts_cleaned.append(my_tokens)\n",
        "    \n",
        "    if counter % round(len(abstracts)/5) == 0:\n",
        "        print(counter, 'abstracts processed')\n",
        "\n",
        "###############################################\n",
        "# read and pre-process gold standard keywords #\n",
        "###############################################\n",
        "\n",
        "path_to_keywords = r\"for_moodle/data/Hulth2003testing/uncontr/\"\n",
        "keywd_names = sorted(os.listdir(path_to_keywords))\n",
        "   \n",
        "keywds_gold_standard = []\n",
        "\n",
        "for counter,filename in enumerate(keywd_names):\n",
        "    # read file\n",
        "    with open(path_to_keywords + filename, 'r') as my_file: \n",
        "        text = my_file.read().splitlines()\n",
        "    text = ' '.join(text)\n",
        "    text =  re.sub('\\s+', ' ', text) # remove formatting\n",
        "    text = text.lower()\n",
        "    # turn string into list of keywords, preserving intra-word dashes \n",
        "    # but breaking n-grams into unigrams\n",
        "    keywds = text.split(';')\n",
        "    keywds = [keywd.strip().split(' ') for keywd in keywds]\n",
        "    keywds = [keywd for sublist in keywds for keywd in sublist] # flatten list\n",
        "    keywds = [keywd for keywd in keywds if keywd not in stpwds] # remove stopwords (rare but may happen due to n-gram breaking)\n",
        "    keywds_stemmed = [stemmer.stem(keywd) for keywd in keywds]\n",
        "    keywds_stemmed_unique = list(set(keywds_stemmed)) # remove duplicates (may happen due to n-gram breaking)\n",
        "    keywds_gold_standard.append(keywds_stemmed_unique)\n",
        "    \n",
        "    if counter % round(len(keywd_names)/5) == 0:\n",
        "        print(counter, 'files processed')\n",
        "\n",
        "##############################\n",
        "# precompute graphs-of-words #\n",
        "##############################\n",
        "\n",
        "### use the terms_to_graph function, store the results in a list named 'gs' ##\n",
        "gs = []\n",
        "for counter in enumerate(abstracts_cleaned):\n",
        "    terms = terms_to_graph(counter[1], 4) #1 car on veut que la partie texte\n",
        "    gs.append(terms)\n",
        "    \n",
        "#gs = [terms_to_graph(abstracts_cleaned[i], 4) for i in range(len(abstracts_cleaned))]\n",
        "    \n",
        "##################################\n",
        "# graph-based keyword extraction #\n",
        "##################################\n",
        "\n",
        "my_percentage = 0.33 # for PR and TF-IDF\n",
        "\n",
        "method_names = ['kc','wkc','pr','tfidf']\n",
        "keywords = dict(zip(method_names,[[],[],[],[]]))\n",
        "\n",
        "for counter,g in enumerate(gs):\n",
        "    # k-core\n",
        "    core_numbers = core_dec(g,False)\n",
        "    ### retain main core as keywords and append the resulting list to 'keywords['kc']' ###\n",
        "    main_core = max(core_numbers.values()) #On récupère la valeur du plus gros poids\n",
        "    keyword = [kwd for kwd, c_n in core_numbers.items() if c_n == main_core] #les keywords seront tous les sommets de poids max\n",
        "    keywords['kc'].append(keyword)\n",
        " \n",
        "    # weighted k-core\n",
        "    core_numbers = core_dec(g,True)\n",
        "    ### retain main core as keywords and append the resulting list to 'keywords['kc']' ###\n",
        "    main_core = max(core_numbers.values())\n",
        "    keyword = [kwd for kwd, c_n in core_numbers.items() if c_n == main_core]\n",
        "    keywords['wkc'].append(keyword)\n",
        "    \n",
        "\n",
        "    # PageRank\n",
        "    pr_scores = zip(g.vs['name'],g.pagerank())\n",
        "    pr_scores = sorted(pr_scores, key=operator.itemgetter(1), reverse=True) # in decreasing order\n",
        "    numb_to_retain = int(len(pr_scores)*my_percentage) # retain top 'my_percentage' % words as keywords\n",
        "    keywords['pr'].append([tuple[0] for tuple in pr_scores[:numb_to_retain]]) \n",
        "    if counter % round(len(gs)/5) == 0:\n",
        "        print(counter)\n",
        "\n",
        "#############################\n",
        "# TF-IDF keyword extraction #\n",
        "#############################\n",
        "\n",
        "abstracts_cleaned_strings = [' '.join(elt) for elt in abstracts_cleaned] # to ensure same pre-processing as the other methods\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=stpwds)\n",
        "### call the .fit_transform() method and name the result 'doc_term_matrix' ###\n",
        "doc_term_matrix = tfidf_vectorizer.fit_transform(abstracts_cleaned_strings)\n",
        "terms = tfidf_vectorizer.get_feature_names() #Les mots\n",
        "vectors_list = doc_term_matrix.todense().tolist()\n",
        "\n",
        "for counter,vector in enumerate(vectors_list):\n",
        "    terms_weights = zip(terms,vector) # bow feature vector as list of tuples\n",
        "    ### keep only non zero values, i.e., the words in the document. store the results in a list named 'nonzero' ##\n",
        "    nonzero = list(terms_weights)\n",
        "    nonzero = [elt for elt in nonzero if elt[1] != 0.0 ]  \n",
        "    nonzero = sorted(nonzero, key=operator.itemgetter(1), reverse=True) # in decreasing order\n",
        "    numb_to_retain = int(len(nonzero)*my_percentage) # retain top 'my_percentage' % words as keywords\n",
        "    keywords['tfidf'].append([tuple[0] for tuple in nonzero[:numb_to_retain]])\n",
        "    \n",
        "    \n",
        "    if counter % round(len(vectors_list)/5) == 0:\n",
        "        print(counter)\n",
        "\n",
        "##########################\n",
        "# performance comparison #\n",
        "##########################\n",
        "\n",
        "perf = dict(zip(method_names,[[],[],[],[]]))\n",
        "for idx,truth in enumerate(keywds_gold_standard):\n",
        "    for mn in method_names:\n",
        "        ### append to the 'perf[mn]' list by using the 'accuracy_metrics' function ###\n",
        "        accuracy = accuracy_metrics(keywords[mn][idx], truth)\n",
        "        perf[mn].append(accuracy)\n",
        "lkgs = len(keywds_gold_standard)\n",
        "\n",
        "# print macro-averaged results (averaged at the collection level)\n",
        "for k,v in perf.items():\n",
        "    print(k + ' performance: \\n')\n",
        "    print('precision:', round(100*sum([tuple[0] for tuple in v])/lkgs,2))\n",
        "    print('recall:', round(100*sum([tuple[1] for tuple in v])/lkgs,2))\n",
        "    print('F-1 score:', round(100*sum([tuple[2] for tuple in v])/lkgs,2))\n",
        "    print('\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 files processed\n",
            "100 files processed\n",
            "200 files processed\n",
            "300 files processed\n",
            "400 files processed\n",
            "0 abstracts processed\n",
            "100 abstracts processed\n",
            "200 abstracts processed\n",
            "300 abstracts processed\n",
            "400 abstracts processed\n",
            "0 files processed\n",
            "100 files processed\n",
            "200 files processed\n",
            "300 files processed\n",
            "400 files processed\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "kc performance: \n",
            "\n",
            "precision: 51.86\n",
            "recall: 62.56\n",
            "F-1 score: 51.55\n",
            "\n",
            "\n",
            "wkc performance: \n",
            "\n",
            "precision: 63.86\n",
            "recall: 48.64\n",
            "F-1 score: 46.52\n",
            "\n",
            "\n",
            "pr performance: \n",
            "\n",
            "precision: 60.18\n",
            "recall: 38.3\n",
            "F-1 score: 44.96\n",
            "\n",
            "\n",
            "tfidf performance: \n",
            "\n",
            "precision: 59.21\n",
            "recall: 38.5\n",
            "F-1 score: 44.85\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEtqyKBbZS3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}